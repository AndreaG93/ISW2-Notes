\documentclass[sigconf]{acmart}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% On windows run updmap
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{dcolumn}
\usepackage{soul}
\usepackage{booktabs} 
\usepackage{pifont}  % use /ding{55} for X... /ding{51} for check
\usepackage{url}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{xspace}
\usepackage{placeins}
\usepackage{boxhandler}
\usepackage[nameinlink]{cleveref}

\expandafter\newcommand\csname r@tocindent4\endcsname{4in}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\lstset{
language=Java,
basicstyle=\small\ttfamily,			
keywordstyle=\color{blue},
commentstyle=\color{gray},			
stringstyle=\color{black},					
}


\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

% My 'newcommand' for modulo function...
\newcommand{\abs}[1]{\left|#1\right|}




% My 'newcommand' for standard table creation...
\newcommand{\AndreaTable}[3]{\begin{table*}[h!]
\caption{#1}
\centering
\begin{tabular}{#2}
\toprule
#3
\bottomrule
\end{tabular}
\end{table*}
}

% My 'newcommand' for standard table creation...
\newcommand{\equivalenceClassesTable}[2]{
\AndreaTable{#1}{llm{8cm}m{8cm}}{#2}
}

\definecolor{beaublue}{rgb}{0.74, 0.83, 0.9}


\settopmatter{printacmref=false, printccs=false, printfolios =false}
\setcopyright{none} 
\renewcommand\footnotetextcopyrightpermission[1]{}
\acmConference[ISW2 Project A.A. 2019-2020]{ }{September 1, 2020}{ }

\begin{document}

\title{ISW2 Project A.A. 2019-2020}

\author{Andrea Graziani ($0273395$)}
\email{andrea.graziani93@outlook.it}
\affiliation{
  \institution{Università degli Studi di Roma "Tor Vergata"}
  \city{Rome}
  \country{Italy}
}

\keywords{Software Defects Prediction, Feature Selection, Sampling, Walk-Forward}
\maketitle
\section{Deliverable 1}

\subsection{Introduction}

How to improve existing software development processes?

Today, in order to accomplish good business results, many organizations around the world, in any industry, adopt a proven set of best practices belonging to a process level improvement training and appraisal program called \textbf{Capability Maturity Model Integration} (\textbf{CMMI})\cite{CMMIINSTITUTE}.

As known\footnote{A properly description of CMMI is out the scope of this report, therefore we assume that reader knows its main features.}, there are five maturity levels defined by that model. For example, at level $3$, \textit{metrics are collected, analysed and used to check development status and to make corrections, if necessary}. At level $4$, measurements are \textit{quantitatively analysed} to control process performance of the project and \textit{to develop a quantitative understanding of the quality of products}, in order to achieve specific quality goals \cite{FLORENCE}.

One of the main tool, adopted to quantitatively analyse a development process, is the \textit{control charts} 
through which all data collected during product life cycle can be analysed using statistical methods\cite{FLORENCE}.

\subsection{Our experiments}

To show the capabilities of this tool, we built an own control chart, reported in \textbf{\cref{ProcessControl}}, showing the bug-fixes number trend over all development cycle of \textbf{Apache Geode™} open-source project\footnote{\url{https://github.com/apache/geode}}.

Control charts are capable to provide en evidence of development process stability, identifying undesirable trends. In fact, if a process is \textit{mature and under statistical process control}, all events should lie within the upper and lower control limits, which are respectively equal to mean $+/-3$ standard deviations. If an event falls out of the control limits the process is said to be out of statistical process control and the reason for this anomaly needs to be investigated for cause and the process brought back under control\cite{FLORENCE}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=10.5cm, height=6cm]{./ProcessControlChart.png}
  \caption{Process control chart representing bug-fixes trend of Apache Geode™ open-source project}
  \label{ProcessControl}
\end{figure}

Fortunately, control chart reported in \textbf{\cref{ProcessControl}} shows that the number of bug fixes is always within the upper and lower limits, therefore, since there are no anomalies, development process can be considered as under statistical process control.

Let's make now some interesting considerations:

\begin{itemize}
\item Why the control chart shows a very low number of number of bug fixes up to middle 2015, reporting, conversely, an important increase towards the end of the same year?
\begin{itemize}
\item Let's start with a brief history. Originally, a software house, called "\textit{GemStone Systems Inc}", developed a product called \texttt{GemFire}, a low latency transactional data engine used in financial contexts, like trading operations.

In May of 2010, \texttt{GemStone} was acquired by "\textit{SpringSource}" which, not long after, was acquired in turn by "\textit{Pivotal}"\footnote{\url{https://en.wikipedia.org/wiki/Pivotal_Software}}.

In 2015, Pivotal proposed the creation of Apache Geode™ to the "\textit{Apache Software Foundation}" to help its development. Apache Geode™ was born.

According to Apache Geode™ commit history, the \textit{first commit} is date on 28 April 2015, which represented a merely import of the source code previously developed by Pivotal. However, the \textit{first release}, is date on 6 February 2016.

Therefore, the reason according to which the control chart shows a very low number of number of bug fixes at the beginning is because, during the early stage of product development, many bugs were been yet undiscovered and, therefore, there were very low bug fixes. In other words, as we will see later, there were many "\textit{dormant defects}".
\end{itemize}

\item Why the control chart shows a very low number of number of bug fixes during 2020?
\begin{itemize}
\item We do \textit{not} know with certainty. According to GitHub logs\footnote{\url{https://github.com/apache/geode/graphs/contributors}}, community contributions started to drop decisively from second half of the year; in fact the number of commits per-day drops under 20 when, normally, it exceeds 30$\setminus$40. 

Therefore, we believe that the reduced development activity is the reason of the trend shown by our control chart.
\end{itemize}

\item Why the control chart shows a very high number of number of bug during 2017?
\begin{itemize}
\item If you look to the number of additions and deletions per week according to GitHub history data\footnote{\url{https://github.com/apache/geode/graphs/code-frequency}}, the number of code lines \textit{deleted} per week reached its peak in 2017, with a $3500$k lines deleted. In the same year, the number of code lines added exceeded $1500$k.

Therefore, aforementioned massive changes into Apache Geode™ source code is probably the reason of the increasing of bugs and, then, of their fixes.

\end{itemize}
\end{itemize}

\subsection{Our implementation}

To avoid boring repetitions, since application logic is very similar to that used for Deliverable 2, all details about our implementation will be described in following section. 

\section{Deliverable 2}

\subsection{Introduction}

How to detect software modules which are more \textbf{defect-prone} than others? Why we are interested to identify them? 

These are some of the most critical questions of software engineering, to which we will try to answer. 

Firstly, a \textbf{software defect} can be generally considered as an error, flaw, bug, mistake, failure, or fault in a computer program or system that may generate an inaccurate or unexpected outcome, or precludes the software from behaving as intended \cite{SoftwareDefectPredictionRawat}.

As known, the presence of defective software modules \textit{always incurs cost in terms of time and money}, because identifying and rectifying defects is one of the most time consuming and expensive software processes \cite{SoftwareDefectPredictionRawat}.

Although it's not practically possible to eliminate each and every defect from a software product, we can reduce their adverse effects exploiting \textbf{software defects prediction} techniques, \textit{which aim is to identify software modules that are defect-prone, in order to reduce the cost of testing activities and code review by letting developers focus on specific artefacts} \citep{Falessi}.

How to build a software defect prediction model? 

In this report, we will describe our software defect prediction model based on \textbf{machine learning} techniques, which building passes through following steps:

\begin{itemize}
\item \textbf{Dataset building}
\item \textbf{Dataset labelling}
\item \textbf{Preprocessing}, using techniques like followings: 
\begin{itemize}
\item \textbf{Features Selection}
\item \textbf{Sampling}
\end{itemize}

\item \textbf{Training and Testing}
\end{itemize}

Technically, there is another step, that is \textbf{classification}, according to which we use our prediction model to classify new data. This step is out the scope of this report.

\subsection{Dataset building}

The first step is, obviously, to \textbf{generate a data-set}, in order to provide an appropriate input for our prediction model; in other words, we need to build a set of \textbf{instances}. 

In our context, each data-set's instance represents a \textbf{source code file} belonging to one of the following open-source software projects which we have analysed:

\begin{description}
\item[Apache BookKeeper\texttrademark]Which is a scalable, fault tolerant and low latency storage service\footnote{\url{https://github.com/apache/bookkeeper}}.
\item[Apache OpenJPA\texttrademark]An implementation of the Java Persistence API specification\footnote{\url{https://github.com/apache/openjpa}}. 
\end{description}

Every data-set's instance is characterized by its values on a fixed, predefined set of \textbf{features}, or \textbf{attributes}, like its code lines number, its age, its complexity and so on. Clearly, in order to build an useful software predictor, \textit{is preferable to collect instances whose features are correlated to the defect-proneness of the software module they belong}. Fortunately, scientific literature reports many studies, about software defect-proneness, which had identified several features (sometimes called also \textbf{metrics}) correlated with defect density of a software module. 

For example, \citet{PredictingDefectsForEclipse} showed that the number of past defects has the highest correlation with number of future defects. \citet{Nagappan} report that \textit{code churn} is a very powerful metric for predict software defect density, while \citet{Gyimothy} showed the same with \textit{lines of code} (\texttt{LOC}) metric.

\subsubsection{Our implementation}
\hfill\\

We extracted these defect-proneness correlated features from \textit{history data} of our projects, exploiting tools like \texttt{git}\footnote{\url{https://git-scm.com/}}, the most famous \textit{version control systems} today available, and \texttt{JIRA}\footnote{\url{https://www.atlassian.com/it/software/jira}}, which is a \textit{issue tracking systems}.

From an implementation point of view, the \texttt{Git} class\footnote{Complete path: \texttt{datasetbuilder.datasources.vcs.git.Git}} contains the entire application logic to manage a \texttt{git} repository, implementing \texttt{Version\-Control\-System} interface, reported in \textbf{\cref{interface1}}). 

To be more precise, this class has following responsibilities: 
\begin{itemize}
\item To identify the commit associated to each project's release.
\item To extract metrics of a given source code file; this is done invoking \texttt{compute\-File\-Metrics} method.
\item To identify all source code files belonging to a release.
\end{itemize}

\begin{lstlisting}[frame=lines,basicstyle=\ttfamily\scriptsize, caption={\texttt{VersionControlSystem} interface}, label={interface1}]
public interface VersionControlSystem {

    Commit getCommitByTag(String tag);

    Commit getCommitByDate(LocalDateTime date);

    Commit getCommitByLogMessagePattern(String pattern);

    Map<String, File> getFiles(String commitHash);

    List<String> getFilesChangedByCommit(String commitHash);

    void computeFileMetrics(File file, Commit releaseCommit);
}
\end{lstlisting}

To be more precise, \texttt{compute\-File\-Metrics} method was been developed in order to extract all metrics reported and described in \textbf{\cref{metrics}}. 

\begin{table}
\caption{Metrics extracted from source code file $X$ by \texttt{compute\-File\-Metrics} method}
\label{metrics}
\begin{tabular}{l|p{4.3cm}}
\toprule
\textbf{Name} & \textbf{Description} \\
\midrule

\texttt{LOC} & Lines of code of $X$. 

\\

\texttt{LOC\_TOUCHED} & $\sum_R (\texttt{addedLOC} + \texttt{deletedLOC})$ \newline Where: 

\begin{itemize}
\item $R$ is the set of all revisions of $X$.
\item $\texttt{addedLOC}$ represents the number of added code lines.
\item $\texttt{deletedLOC}$ represents the number of removed code lines.
\end{itemize}

\\

\texttt{LOC\_ADDED} & $\sum_R (\texttt{addedLOC})$

\\

\texttt{MAX\_LOC\_ADDED} & $\max_R (\texttt{addedLOC})$.

\\
    
\texttt{AVERAGE\_LOC\_ADDED} & $\dfrac{\sum_R (\texttt{addedLOC})}{\mid R \mid}$

\\

\texttt{NUMBER\_OF\_REVISIONS} & $\mid R \mid$ 

\\

\texttt{NUMBER\_OF\_FIX} & Number of bug fixes which have involved $X$.

\\

\texttt{NUMBER\_OF\_AUTHORS} & The authors number which developed file $X$.

\\

\texttt{CHURN} & $\sum_R (\texttt{addedLOC} - \texttt{deletedLOC})$

\\

\texttt{MAX\_CHURN} & $\max_R (\texttt{addedLOC} - \texttt{deletedLOC})$

\\

\texttt{AVERAGE\_CHURN} & $\dfrac{\sum_R (\texttt{CHURN})}{\mid R \mid}$

\\

\texttt{MAX\_CHANGE\_SET\_SIZE} & The number of files committed together with $X$.

\\

\texttt{AVERAGE\_CHANGE\_SET\_SIZE} & $\dfrac{\texttt{MAX\_CHANGE\_SET\_SIZE}}{\mid R \mid}$

\\

\texttt{AGE\_IN\_WEEKS} & Age of $X$ in weeks.

\\

\texttt{WEIGHTED\_AGE\_IN\_WEEKS} & $\dfrac{\texttt{AGE\_IN\_WEEKS}}{\texttt{LOC\_TOUCHED}}$

\\

\bottomrule
\end{tabular}
\end{table}

\subsection{Dataset labelling}

To be precise, our aim is to build a software defect predictor using a \textbf{supervised learning scheme}.

When a supervised learning scheme is used, the goal of our predictor is \textit{to learn a function that maps an input to an output based on example input-output pairs}. In other word, each data-set instance $T$ is represented as a pair $(x_{d}, y)$ where $x_d$ is a $d$-dimensional input object while $y$ is a \textit{tag}, or a \textit{label}, representing the \textit{desired output value}. In this way, a supervised learning algorithm, analysing \textit{labelled training data}, can produce an inferred function, which can be used for mapping \textit{unlabelled data}.

Therefore, the second very important step to build our predictor is the so-called \textit{labelling} task, according to which one or more informative tags must be assigned to each unlabelled instance of a given dataset.

\subsubsection{Our implementation}
\hfill\\

In our context, to build a properly data-set, we need to label each instance as \textit{buggy} or \textit{not buggy}. 

First of all, it is necessary to identify all project's releases. 

\texttt{Jira} class\footnote{Complete path: \texttt{datasetbuilder.datasources.its.jira.Jira}} contains application logic used to query \texttt{Jira} servers on the cloud. It implements the \texttt{Issue\-Tracking\-System} interface, shown in \textbf{\cref{interface2}}, providing the implementations for following methods:

\begin{description}
\item[\texttt{getReleases}] Which is used to retrieve all necessary information about project's releases, including names, release dates and so on.
\item[\texttt{getIssuesRegistry}] Invoked to get all data about known issues 
\end{description}

\begin{lstlisting}[frame=lines,basicstyle=\ttfamily\scriptsize, caption={\texttt{IssueTrackingSystem} interface}, label={interface2}]
public interface IssueTrackingSystem {

    List<Release> getReleases(String projectName);

    IssueRegistry getIssuesRegistry(
    	String projectName, 
    	Map<Integer, Release> releasesByVersionID);
}
\end{lstlisting}

Querying \texttt{Jira} is not enough, because to proceed we need all commits associated to each release.

To do that, we have exploited \texttt{git} tags inside commits messages to identify the commit associated to each release. 

However, when sometimes \texttt{git} tags were not available, we have identified aforementioned commits exploiting \texttt{git\- log} command with \texttt{--before\- [date]} options. The application logic to do that, is contained into \texttt{Git} class.

At this point, knowing commit associated to each release, we are able to retrieve all source code files associated to each project's release, however we still have to label them.

To do that we need to discover all known project's issues and, to do that, we queried once again \texttt{Jira} to obtain the identifiers of all issues with type \texttt{"Bug"} and status \texttt{"Closed"} (or \texttt{"resolved"}). We remember that is possible to find the implementation of this query procedure inside \texttt{get\-Issues\-Registry} method into \texttt{Jira} class. Please note that, the same logic was used to build all necessary data for Deliverable 1 too.

Finally, to label each source code file as buggy or not, we need to check if the release, which the file belongs to, is an "\textit{Affected Version}" (\textbf{AV}).

Is very important to precise that scientific literature reports that most of the projects have more than $25\%$ of defects whose issue report contained no information about affected versions. Moreover, the vast majority of all bugs ($51\%$) resulted in not having or having an unreliable AV. Therefore, if information about the AV for a bug is available in \texttt{Jira}, we considered the earliest AV in \texttt{Jira} to be the release that introduced the bug, otherwise we have adopted the so-called \textbf{proportion method}.

\paragraph{The dormant defects problem}
\hfill\\
Empirical results show that is very common that a defect is discovered several releases \textit{after} its introduction \citep{Falessi}. 

This fact implies that if we observe, today, the status of a source code file in its current version, it can be considered as defect-free while, in reality, this conclusion is \textit{not} true because it can contain \textit{undiscovered defects}.

In according to scientific literature, these undiscovered defects, commonly called "\textit{dormant defects}" too, can negatively impacts our classifier's accuracy. We call "\textit{snoring}" the "\textit{noise}" consisting of all source code file containing dormant defects \citep{Falessi}.

In order to remedy, literature suggest \textbf{to remove the most recent releases from a dataset}. In fact, since earlier releases likely contain more snoring classes than older releases, removing the most recent is possible to reduce the snoring effect improving our classifiers evaluations. Since scientific literature states that the first 50\% of releases are not heavily impacted by snoring \citep{Falessi}, for every analysed project, we have choose to remove from our dataset half of available releases. From an implementation point of view, this task is done executing \texttt{retrieve\-Releases\-Discarding\-HalfOfThem} method, which belongs to \texttt{Project\-Dataset\-Builder} class \footnote{Complete path: \texttt{datasetbuilder.ProjectDatasetBuilder}}.

\subsection{Preprocessing}

\subsubsection{Feature selection}
\hfill\\
After generating our data-set with metrics and labels, we applied a preprocessing technique called \textbf{feature selection} or \textbf{attribute selection}.

As known, most machine learning algorithms are designed to learn which are the most appropriate attributes to use for making their decisions. For example, decision tree methods choose the most promising attribute to split on at each point and should never select irrelevant or unhelpful attributes \cite{FalessiDataMining}. Although, theoretically, having more attributes give us a more discriminating power, in practice, adding irrelevant attributes to a dataset, performances of machine learning systems degrades \cite{FalessiDataMining}. 

Therefore, using a feature selection technique, deleting unsuitable attributes, is possible to \textit{improve learning algorithms performances}. However, using specific data-sets, is possible that they \textit{may require extensive computations}, degrading overall performances \cite{FalessiDataMining}.

How to select relevant features?

Clearly, the best way to select relevant attributes is \textit{manually}, based on a deep understanding of the learning problem. However, fortunately, automatic methods exist too. 

For our analysis, we have adopted \textbf{Best-first} method. This scheme adopts a \textit{greedy approach} to select an attribute subset that is most likely to make better predictions. Its peculiarity it that it does not just terminate when the performance starts to drop but keeps a list of all attribute subsets evaluated so far, sorted in order of the performance measure, so that it can revisit an earlier configuration instead \cite{FalessiDataMining}. 

At this point, our aim is to evaluate the advantages deriving from using a feature selection technique like \textit{Best-First}. How to do that?

\paragraph{How to evaluate classifier accuracy}
\hfill\\

According to project specifications, we have used following classifiers, which implementation is provided by \texttt{Weka}\footnote{\texttt{https://www.cs.waikato.ac.nz/ml/weka/}}, an open source machine learning software:
\begin{itemize}
\item \textbf{Random Forest}
\item \textbf{IBK}
\item \textbf{Naive Bayes}
\end{itemize}

Our classifier's accuracy evaluation method consists of measuring following parameter:

\begin{description}

\item[\textbf{Recall}] also known as \textbf{true positive rate}, measures correctly predicted buggy instances among all buggy instances:

\begin{equation}
\dfrac{TP}{TP + FN}
\end{equation}

\item[\textbf{Precision}] which tell us many times our classifier has correctly classified an instance as buggy:

\begin{equation}
\dfrac{TP}{TP + FP}
\end{equation}

\item[\textbf{AUC}] measures the area under the \textit{receiver operating characteristic} (\textbf{ROC}) curve, plotted using \textit{false positive rate} and \textit{true positive rate} together.

\item[\textbf{Kappa}] tell to us how many times our classifier was been more accurate than a dummy classifier:

\begin{equation}
\dfrac{\text{Accuracy of classifier} - \text{Accuracy of dummy classifier}}{1 - \text{Accuracy of dummy classifier}}
\end{equation}

\end{description}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{./BOOKKEEPER/FeatureSelection.png}
  \caption{Comparison of classifiers accuracy when using a feature selection algorithm. No sampling technique applied. \newline BookKeeper™ project. \newline}
  \label{BOOKKEEPER-FeatureSelection}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{./OPENJPA/FeatureSelection.png}
  \caption{Comparison of classifiers accuracy when using a feature selection algorithm. No sampling technique applied.\newline OpenJPA™ project. \newline}
  \label{OPENJPA-FeatureSelection}
\end{figure*}

\FloatBarrier

\paragraph{Evaluation results}
\hfill\\

Comparing results derived by processing our data-sets using best-first technique with those processed without feature selection algorithms, we have noted that \textit{Best-First} method is capable to \textbf{improve the accuracy of specific classifiers}.
 
As you can observe from \textbf{\cref{BOOKKEEPER-FeatureSelection}}, using our BookKeeper™ dataset, when a \textit{Naive Bayes} classifier is used, exploiting \textit{Best-First} method without any sampling scheme (which we will describe in next section), we have obtained following results:
\begin{itemize}
\item \textit{Recall} parameter increased by $27.72\%$, passing from a value equal to $0.440$ to $0.562$. 
\item \textit{Kappa} parameter increased by $40.22\%$, from $0.179$ to $0.251$. 
\item \textit{ROC} parameter increased by $2.47\%$, from $0.686$ to $0.703$.
\item \textit{Precision} parameter decrease by $1.94\%$, from $0.630$ to $0.618$.
\end{itemize}

Our results confirm that \textit{Naive Bayes} classifier improves its accuracy when \textit{Best-First} method is used. 

However, using others classifiers, we have obtained very different results. In fact, when \textit{Random Forest} classifier is used, our results show that: 

\begin{itemize}
\item \textit{Recall} parameter decreased by $12,5\%$, passing from a value equal to $0.612$ to $0.544$. 
\item \textit{Kappa} parameter decreased by $25.28\%$, from $0.446$ to $0.356$. 
\item Both \textit{ROC} and \textit{Precision} parameters decreased by $2.87\%$ and $4.84\%$ respectively.
\end{itemize}

In other words, when \textit{Random Forest} classifier is used, its accuracy get worse when \textit{Best-First} method is applied. \textit{IBK} classifier shows results very similar to those of \textit{Random Forest}. Despite some differences, using OpenJPA™ dataset, results are similar. 

\subsubsection{Sampling}
\hfill\\

As known, performance of a software defect prediction model depends heavily on the data on which it was trained.

In fact, when a defect prediction model is trained on \textbf{imbalanced datasets}, that is using a datasets where the proportion of defective and clean modules is not equally represented, it is \textit{highly susceptible to producing inaccurate prediction models} \cite{FalessiSampling}.

To mitigate the risk of an imbalanced dataset, is possible to use several \textit{sampling} techniques. In compliance with the project specifications, we have adopted following techniques.

\begin{description}
\item[Over-sampling] It randomly samples with replacement the minority class to be the same size as the majority class. It is also known as \textit{up-sampling}.

\item[Under-sampling] It samples the majority class in order to reduce the number of majority modules to be the same number as the minority class. It is also known as \textit{down-sampling}.

\item[SMOTE] This technique combats the disadvantages of both over-sampling and under-sampling techniques, creating artificial data based on the feature space similarities from the minority class. 

\end{description}

\paragraph{Our implementation}
\hfill\\
The implementations of aforementioned sampling techniques are provided, once again, by \texttt{Weka} software.

To measure advantages deriving from using a sampling method, we evaluated our classifiers accuracy both when a sampling technique is exploited and when no sampling is used. In this case, no feature selection algorithm was been used.

\begin{itemize}

\item Using BookKeeper™ dataset,our results, shown in \textbf{\cref{BOOKKEEPER-Sampling}}, show that:
\begin{itemize}

\item Regardless of the classifier used, \textit{Recall} parameter improves always when any sampling technique is applied. In particular, \textit{Random Forest} classifier using \textit{under-sampling} achieves the highest \textit{Recall} value, increasing by $26.63\%$ the value achieved by the same classifier when no sampling is used.

\item Generally, \textit{Precision} parameter gets little worse when any sampling technique is applied. The worst results are achieved using \textit{Naive Bayes} classifier with \textit{under-sampling} technique, decreasing by $2.77\%$ the value achieved by the same classifier when no sampling is used.

\item \textit{ROC} parameter gets very little worse when any sampling technique is applied too. The only exception is represented by \textit{Naive Bayes} classifier with \textit{SMOTE} technique which perform slightly better than when no sampling is used.

\item Regarding \textit{Kappa} parameter, \textit{Naive Bayes} performs very well when any sampling technique is applied, reaching best results when \textit{SMOTE} technique is used. Other classifier perform generally worse. The worst results are achieved using \textit{IBK} classifier with \textit{over-sampling} technique.

\end{itemize}


\item Using OpenJPA™ dataset, our results, shown in \textbf{\cref{OPENJPA-Sampling}}, show that:
\begin{itemize}

\item Like previously, regardless of the classifier used, \textit{Recall} parameter improves always. In particular, \textit{Random Forest} classifier using \textit{under-sampling} achieves the highest \textit{Recall} value, increasing by $64.63\%$ the value achieved by the same classifier when no sampling is used; a very good results.

\item Generally, \textit{Precision} parameter gets little worse when any sampling technique is applied. All classifier perform very bad when \textit{under-sampling} is used. The only exception is represented by \textit{Naive Bayes} classifier with \textit{SMOTE} technique which perform slightly better than when no sampling is used.

\item \textit{ROC} parameter gets very little worse when any sampling technique is applied too. The only exception is represented by \textit{Random Forest} classifier with \textit{SMOTE} technique which performs better.

\item Using OpenJPA™ dataset, regarding \textit{Kappa} parameter, all classifier perform very bad when \textit{under-sampling} is used. \textit{Naive Bayes} performs better when \textit{SMOTE} is used. In all other case, worse results are achieved.

\end{itemize}
\end{itemize}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{./BOOKKEEPER/Sampling.png}
  \caption{Comparison of classifiers accuracy when using sampling algorithms. No feature selection technique applied. \newline BookKeeper™ project. \newline}
  \label{BOOKKEEPER-Sampling}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{./OPENJPA/Sampling.png}
  \caption{Comparison of classifiers accuracy when using sampling algorithms. No feature selection technique applied.\newline OpenJPA™ project. \newline\newline\newline\newline\newline}
  \label{OPENJPA-Sampling}
\end{figure*}

\FloatBarrier

\subsection{Training and Testing}

At this point we can train our software defect prediction model but, firstly, we will talk about \textbf{validation techniques}. Why are they so important?

Validation techniques describe a specific way to split available data in \textit{training set} and \textit{test set} and they are extremely important to do our evaluations. 

First of all, is very important to emphasize that the data-set involved during training phase \textit{cannot} be used to measure above parameters due to following reasons:
\begin{itemize}

\item We \textit{already} know the classifications of each instance belonging to training set, therefore we are not interested in accuracy about predictions involving these data.

\item Any estimate of performance based on training data will be, clearly, optimistic and, therefore, not very useful.
\end{itemize}

Therefore, to evaluate classifier's performance, we have to use a new data-set, containing instances \textit{not} used during training phase; this independent dataset is the test set. To identify our test set we need to use a validation technique. 

\subsubsection{The walk-forward validation technique}
\hfill\\

In accordance to project's specifications, we have adopted a very common validation technique called \textbf{Walk-forward}. Precisely, it is a \textit{time-series} validation technique because it has the ability to \textit{preserve the temporal order of data}, preventing that testing set have data antecedent to the training set.

According to walk-forward technique, a data-set must be split into parts, which must be the smallest units that can be \textit{chronologically} ordered. Therefore, during the development of our project, our data-set was been divided into parts containing all source code files belonging to each project's release, the latter are been chronologically ordered respect to their release date.

Then and in each run, all data available before the part to predict is used as the training set, and the part to predict is used as test-set. Afterwards, the model accuracy is computed as the average among runs. The number of runs is equal to one less than the number of parts.

For example, regarding \textbf{BookKeeper}™ project, we have built a dataset containing $6$ releases, therefore all data available were divided into $6$ parts, while the number of runs is equal to $5$.

Particularly, during first run, the first part is used as a training, and the second as testing. During the second run the first two parts are used as a training and the third as testing, and so on. 
 
\paragraph{Some experiments} 
\hfill\\
At this point, we wanted to know how accuracy's parameters behave according to the number of releases used for training using walk-forward validation technique. Results, reported in \textbf{\cref{BOOKKEEPER-History}} and \textbf{\cref{OPENJPA-History}}, show that:

\begin{itemize}
\item Regarding BookKeeper™ project, regardless of the classifier used, all parameters improve linearly, increasing the number of releases used for training. Better results are achieved using $6$ releases during training phase.  

\item Regarding OpenJPA™ project, the graph shows a very irregular trend about all parameters. However, regardless of the classifier used, better results are achieved using $9$ releases, however they start to get worse immediately after.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=10.5cm, height=6cm]{./BOOKKEEPER/History.png}
  \caption{Trend of the accuracy's parameters respect to the releases number. \newline BookKeeper™ project. \newline}
  \label{BOOKKEEPER-History}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=10.5cm, height=6cm]{./OPENJPA/History.png}
  \caption{Trend of the accuracy's parameters respect to the releases number. \newline OpenJPA™ project.}
  \label{OPENJPA-History}
\end{figure}

\paragraph{Final comparison} 
\hfill\\
From accuracy point of view, what is the best classifier? What sampling or feature selection algorithm perform better with our datasets?

To reply to these question, we have to simply observer \textbf{\cref{BOOKKEEPER-Total}} and \textbf{\cref{OPENJPA-Total}}, which contain a summary evaluation of all possible combination of algorithms and classifiers using BookKeeper™ and OpenJPA™ datasets respectively.

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{./BOOKKEEPER/Total.png}
  \caption{Summary of accuracy of our classifiers. \newline BookKeeper™ project. \newline}
  \label{BOOKKEEPER-Total}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{./OPENJPA/Total.png}
  \caption{Summary of accuracy of our classifiers. \newline OpenJPA™ project. \newline\newline\newline\newline\newline}
  \label{OPENJPA-Total}
\end{figure*}

\clearpage

\section{Links}

\begin{table}[!ht]
\small
\begin{tabular}{l}

\url{https://github.com/AndreaG93/ISW2-Reports} \\

\url{https://github.com/AndreaG93/Deliverable-1-ISW2-Project} \\

\url{https://github.com/AndreaG93/Deliverable-2-ISW2-Project} \\

\url{https://travis-ci.org/github/AndreaG93/Deliverable-1-ISW2-Project} \\

\url{https://travis-ci.org/github/AndreaG93/Deliverable-2-ISW2-Project} \\

\url{https://sonarcloud.io/dashboard?id=AndreaG93_Deliverable-1-ISW2-Project} \\

\url{https://sonarcloud.io/dashboard?id=AndreaG93_Deliverable-2-ISW2-Project} \\

\end{tabular}
\end{table}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\end{document}
\endinput


