\documentclass[10pt,a4paper]{article}

\usepackage[a4paper, total={8in, 10in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=black,bookmarksopen=true]{hyperref}
\usepackage{bookmark}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{quoting}

\quotingsetup{font=small}

\lstset{
language=C,
basicstyle=\small\ttfamily,			
keywordstyle=\color{blue},
commentstyle=\color{gray},			
stringstyle=\color{black},			
numbers=left,						
numberstyle=\tiny,					
stepnumber=1,						
breaklines=true						
}

\begin{document}

\section{Falessi}

\subsection{"Leveraging Defects Lifecycle for Labeling Defective Classes"}

\textbf{Software Defect Prediction} (SDP) is one of the most useful and cost efficient activity \textit{used to identify software modules that are defect prone and require extensive testing in order to use testing resources efficiently.} 

Software defect predictors are based on statistical approach and, particularly, machine learning. However Some research studies adopted recent machine learning techniques such as active/semi-supervised learning to improve prediction performance. Prediction models learned by machine learning algorithms can predict either bug-proneness of source code (classification) or the number of defects in source code (regression).

The software defect prediction process based on \textbf{machine learning models} requires some steps:

\begin{enumerate}

\item First of all is necessary to generate the \textbf{instances} from software archives for our prediction model. Instances can be generated from version control systems, issue tracking systems, e-mail archives, and so on. Each instance represents a system, a software component, a source code file, a class, a function and so on.

\textit{An instance can have several metrics (or features) extracted from the software archives and is labeled with buggy/clean or the number of bugs.}

\item After generating instances with metrics and labels, we can apply preprocessing techniques, which are common in machine learning. Preprocessing techniques used in defect prediction studies include \textbf{feature selection}, \textbf{data normalization}, and \textbf{noise reduction}. However preprocessing is an optional step.  

\item Finally we have to train a prediction model in such a way it can predict whether a new instance has a bug or not. \textit{The prediction for bug-proneness (buggy or clean) of an instance stands for binary classification, while that for the number of bugs in an instance stands for regression.}

\end{enumerate}

\textbf{Clearly source code metrics measure how source code is complex and the main rationale of the source code metrics is that source code with higher
complexity can be more bug-prone.} 

\textbf{Process metrics are extracted from software archives such as version control systems and issue tracking systems.}

To measure defect prediction results by classification models, we should consider the following prediction outcomes first:

\begin{description}

\item[True Positive (TP)] buggy instances predicted as buggy.
\item[False positives (FP)] clean instances predicted as buggy.
\item[True negative (TN)] clean instances predicted as clean.
\item[False negative (FN)] buggy instances predicted as clean.

\end{description}

With these outcomes, we can define the following measures, which are mostly used in the defect prediction literature.

\begin{description}

\item[Precision]:

\begin{equation}
\dfrac{TP}{TP + FP}
\end{equation}

\item[Recall] Recall, also know as \textbf{probability of detection} (\textbf{PD}) or \textbf{true positive rate} (\textbf{TPR}), measures correctly predicted buggy instances among all buggy instances:

\begin{equation}
\dfrac{TP}{TP + FN}
\end{equation}

\item[F-measure] F-measure is a harmonic mean of precision and recall:

\begin{equation}
\dfrac{2 \times (Precision \times Recall)}{Precision + Recall}
\end{equation}


\end{description}

Although bug prediction is an important tool, it was not possible to study the origin of bugs in large-scale scenarios until the introduction of the \textbf{SZZ algorithm}, which was proposed by Sliwerski, Zimmermann and Zeller - hence the acronym. 

The SZZ algorithm \textit{traces back the code history to find changes that are likely to introduce bugs}, i.e., the so-called \textbf{bug-introducing changes}. 

However, SZZ is not without limitations because it may produce inaccurate data by not recognizing that bugfix changes may contain interleaved refactorings, since code refactoring does not directly fix a bug. Similarly, SZZ may erroneously flag refactoring changes as bug-introducing changes. 

SZZ is an algorithm used to identify bug-introducing changes. In order to identify bug-introducing changes, the SZZ algorithm starts analyzing the bug-fix changes, which are changes that are known to fix a bug that is reported on an Issue Tracking System (e.g. JIRA and Bugzilla).

Using change logs provided by VCSs, the SZZ algorithm identifies a bug ID and the bug-fix change. 

Next, SZZ performs a \texttt{diff} operation between the bug-fix change and a previous change to identify how the bug was fixed. 

Finally, to locate the bug-introducing change, SZZ traces back in code history (e.g., using the \texttt{git blame} function) to find the change that introduced the bug, the bug-introducing change.


\end{document}






















